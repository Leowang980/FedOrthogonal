{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "from nets.cnn import CNNCifar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(CNNCifar,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "        self.linear = nn.Linear(16, 512)\n",
    "        self.orthogonal = nn.utils.parametrizations.orthogonal(nn.Linear(16, 512))\n",
    "\n",
    "    def forward_feature(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=F.relu(x)\n",
    "        x=F.max_pool2d(x,2)\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride, track):\n",
    "        super(Block, self).__init__()\n",
    "        norm1=nn.BatchNorm2d(in_planes, momentum=None, track_running_stats=track)\n",
    "        norm2=nn.BatchNorm2d(planes, momentum=None, track_running_stats=track)\n",
    "        self.norm1=norm1\n",
    "        self.conv1=nn.Conv2d(in_planes, planes, 3, padding=1, stride=stride, bias=False)\n",
    "        self.norm2=norm2\n",
    "        self.conv2=nn.Conv2d(planes, planes, 3, padding=1, stride=1, bias=False)\n",
    "        self.shortcut=nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut=nn.Conv2d(in_planes, planes, 1, stride, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.norm1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.norm2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, model_rate, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        pre_hidden_size=[64, 128, 256, 512]\n",
    "        hidden_size=[int(np.ceil(i*model_rate))  for i in pre_hidden_size]\n",
    "        self.in_planes = hidden_size[0]\n",
    "        self.conv1=nn.Conv2d(3, hidden_size[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.layer1=self._make_layer(block, hidden_size[0], num_blocks[0], stride=1, track=False)\n",
    "        self.layer2=self._make_layer(block, hidden_size[1], num_blocks[1], stride=2, track=False)\n",
    "        self.layer3=self._make_layer(block, hidden_size[2], num_blocks[2], stride=2, track=False)\n",
    "        self.layer4=self._make_layer(block, hidden_size[3], num_blocks[3], stride=2, track=False)\n",
    "        self.output=nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_size[-1], num_classes)\n",
    "        )\n",
    "        #self.linear=nn.Linear(hidden_size[-1], num_classes)\n",
    "            \n",
    "    def _make_layer(self, block, planes, num_blocks, stride, track):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = list()\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, track))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def extract_feature(self, x):\n",
    "        x=self.conv1(x)\n",
    "        feat1=self.layer1(x)\n",
    "        feat2=self.layer2(feat1)\n",
    "        feat3=self.layer3(feat2)\n",
    "        feat4=self.layer4(feat3)\n",
    "        out=self.output(feat4)\n",
    "        feat1=self.layer2[0].norm1(feat1)\n",
    "        feat2=self.layer3[0].norm1(feat2)\n",
    "        feat3=self.layer4[0].norm1(feat3)\n",
    "        return [feat1, feat2, feat3, feat4], out\n",
    "    \n",
    "    def forward_feature(self, x):\n",
    "        out=self.conv1(x)\n",
    "        out=self.layer1(out)\n",
    "        out=self.layer2(out)\n",
    "        out=self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        #print(out.shape)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def forward_head(self, x):\n",
    "        out=self.output(x)\n",
    "        return out \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.forward_feature(x)\n",
    "        out=self.forward_head(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def ResNet18Cifar(model_rate):\n",
    "    model = ResNet(Block, model_rate, num_blocks=[2, 2, 2, 2], num_classes=10)\n",
    "    return model\n",
    "\n",
    "def ResNetCifar(model, model_rate):\n",
    "    if model=='resnet18':\n",
    "        return ResNet18Cifar(model_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train=transforms.Compose([  \n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])  \n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def cifar10_global(batch_size,root):\n",
    "    dataset_train=datasets.CIFAR10(root, train=True, transform= transform_train, download=True)\n",
    "    dataset_test=datasets.CIFAR10(root, train=False, transform= transform_test, download=True)\n",
    "    dataloader_train=data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test=data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher_model=ResNetCifar('resnet18',1.0)\n",
    "student_model=CNNCifar()\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "batch_size=128\n",
    "dataloader_train_global, dataloader_test_global=cifar10_global(batch_size, root='../../data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 16, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i, (data, target) in enumerate(dataloader_train_global):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    print(data.shape)\n",
    "    feature=student_model.forward_feature(data)\n",
    "    print(feature.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.8175395471055795, acc: 0.4283\n",
      "Epoch 2/50, Loss: 1.5708941858442849, acc: 0.4832\n",
      "Epoch 3/50, Loss: 1.4593743524892862, acc: 0.5164\n",
      "Epoch 4/50, Loss: 1.396874821704367, acc: 0.527\n",
      "Epoch 5/50, Loss: 1.3403127821510101, acc: 0.5508\n",
      "Epoch 6/50, Loss: 1.3004061933368674, acc: 0.5775\n",
      "Epoch 7/50, Loss: 1.2585096715966149, acc: 0.5797\n",
      "Epoch 8/50, Loss: 1.2262680600671207, acc: 0.6026\n",
      "Epoch 9/50, Loss: 1.201399844168397, acc: 0.5917\n",
      "Epoch 10/50, Loss: 1.1774942980093115, acc: 0.6002\n",
      "Epoch 11/50, Loss: 1.159031412638057, acc: 0.608\n",
      "Epoch 12/50, Loss: 1.148664189574054, acc: 0.6138\n",
      "Epoch 13/50, Loss: 1.1229780852947089, acc: 0.6237\n",
      "Epoch 14/50, Loss: 1.1104651183423484, acc: 0.6314\n",
      "Epoch 15/50, Loss: 1.0990773965330685, acc: 0.6414\n",
      "Epoch 16/50, Loss: 1.0854847303131963, acc: 0.6241\n",
      "Epoch 17/50, Loss: 1.0819178382149133, acc: 0.6466\n",
      "Epoch 18/50, Loss: 1.0638349330638681, acc: 0.6431\n",
      "Epoch 19/50, Loss: 1.055958756095613, acc: 0.6393\n",
      "Epoch 20/50, Loss: 1.0473766861974125, acc: 0.6519\n",
      "Epoch 21/50, Loss: 1.0380818685302344, acc: 0.6473\n",
      "Epoch 22/50, Loss: 1.0321294314721052, acc: 0.6462\n",
      "Epoch 23/50, Loss: 1.0277019768114894, acc: 0.6579\n",
      "Epoch 24/50, Loss: 1.0214974425942696, acc: 0.6616\n",
      "Epoch 25/50, Loss: 1.013060510768305, acc: 0.6566\n",
      "Epoch 26/50, Loss: 1.0038245587092836, acc: 0.6518\n",
      "Epoch 27/50, Loss: 1.0029945786651748, acc: 0.674\n",
      "Epoch 28/50, Loss: 0.9898784323726468, acc: 0.6625\n",
      "Epoch 29/50, Loss: 0.9965342304590717, acc: 0.6704\n",
      "Epoch 30/50, Loss: 0.9856917147746171, acc: 0.6769\n",
      "Epoch 31/50, Loss: 0.987531473569553, acc: 0.6663\n",
      "Epoch 32/50, Loss: 0.9716893402511811, acc: 0.6669\n",
      "Epoch 33/50, Loss: 0.9762939522638345, acc: 0.6783\n",
      "Epoch 34/50, Loss: 0.9694414346114449, acc: 0.6724\n",
      "Epoch 35/50, Loss: 0.9677777201928142, acc: 0.6855\n",
      "Epoch 36/50, Loss: 0.9618314259192523, acc: 0.6807\n",
      "Epoch 37/50, Loss: 0.9531975582127681, acc: 0.6809\n",
      "Epoch 38/50, Loss: 0.9497294037238412, acc: 0.687\n",
      "Epoch 39/50, Loss: 0.9531844613497215, acc: 0.6819\n",
      "Epoch 40/50, Loss: 0.9457676651532693, acc: 0.6776\n",
      "Epoch 41/50, Loss: 0.9436947322257644, acc: 0.6791\n",
      "Epoch 42/50, Loss: 0.9355760602390065, acc: 0.6751\n",
      "Epoch 43/50, Loss: 0.93401807881987, acc: 0.6819\n",
      "Epoch 44/50, Loss: 0.9334945021687872, acc: 0.6896\n",
      "Epoch 45/50, Loss: 0.931973686303629, acc: 0.6805\n",
      "Epoch 46/50, Loss: 0.9255208760271292, acc: 0.6768\n",
      "Epoch 47/50, Loss: 0.9238168056053884, acc: 0.6849\n",
      "Epoch 48/50, Loss: 0.9181140185621999, acc: 0.6817\n",
      "Epoch 49/50, Loss: 0.9235197727942406, acc: 0.6804\n",
      "Epoch 50/50, Loss: 0.9205808772150513, acc: 0.6824\n"
     ]
    }
   ],
   "source": [
    "def train_student(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        test_acc = test(model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "train_student(student_model, dataloader_train_global, 50, nn.CrossEntropyLoss(), torch.optim.Adam(student_model.parameters(), lr=0.001), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.7737682713267138, acc: 0.4409\n",
      "Epoch 2/50, Loss: 1.222518632174148, acc: 0.569\n",
      "Epoch 3/50, Loss: 0.9864564073055296, acc: 0.6285\n",
      "Epoch 4/50, Loss: 0.8406389271816634, acc: 0.6828\n",
      "Epoch 5/50, Loss: 0.7154518311743236, acc: 0.7263\n",
      "Epoch 6/50, Loss: 0.6263538815481279, acc: 0.7624\n",
      "Epoch 7/50, Loss: 0.5537493228912354, acc: 0.7834\n",
      "Epoch 8/50, Loss: 0.5022377313860237, acc: 0.8096\n",
      "Epoch 9/50, Loss: 0.454133230135264, acc: 0.8129\n",
      "Epoch 10/50, Loss: 0.41729632820314766, acc: 0.8224\n",
      "Epoch 11/50, Loss: 0.38236659349840313, acc: 0.8264\n",
      "Epoch 12/50, Loss: 0.3600789384006539, acc: 0.8325\n",
      "Epoch 13/50, Loss: 0.32946471183958564, acc: 0.8457\n",
      "Epoch 14/50, Loss: 0.30436257850331117, acc: 0.8441\n",
      "Epoch 15/50, Loss: 0.2860005771755562, acc: 0.8508\n",
      "Epoch 16/50, Loss: 0.2703135870492367, acc: 0.8614\n",
      "Epoch 17/50, Loss: 0.24511759587184853, acc: 0.8658\n",
      "Epoch 18/50, Loss: 0.24065172169214624, acc: 0.8657\n",
      "Epoch 19/50, Loss: 0.22084292727510643, acc: 0.857\n",
      "Epoch 20/50, Loss: 0.20665774528708908, acc: 0.8696\n",
      "Epoch 21/50, Loss: 0.19292318003485576, acc: 0.8722\n",
      "Epoch 22/50, Loss: 0.18351071159286267, acc: 0.8766\n",
      "Epoch 23/50, Loss: 0.1707723449220133, acc: 0.8757\n",
      "Epoch 24/50, Loss: 0.15917059731529193, acc: 0.878\n",
      "Epoch 25/50, Loss: 0.15554759462776085, acc: 0.8815\n",
      "Epoch 26/50, Loss: 0.14541729662062414, acc: 0.8664\n",
      "Epoch 27/50, Loss: 0.14486248624008483, acc: 0.8811\n",
      "Epoch 28/50, Loss: 0.12700677036648364, acc: 0.8708\n",
      "Epoch 29/50, Loss: 0.11862495874562075, acc: 0.8769\n",
      "Epoch 30/50, Loss: 0.11700270989971698, acc: 0.8825\n",
      "Epoch 31/50, Loss: 0.10788913279333535, acc: 0.8853\n",
      "Epoch 32/50, Loss: 0.10184587262894797, acc: 0.8886\n",
      "Epoch 33/50, Loss: 0.10147595661870963, acc: 0.8872\n",
      "Epoch 34/50, Loss: 0.09162204410128123, acc: 0.8869\n",
      "Epoch 35/50, Loss: 0.09021616162127241, acc: 0.8839\n",
      "Epoch 36/50, Loss: 0.08171284352631672, acc: 0.8893\n",
      "Epoch 37/50, Loss: 0.08784633367548665, acc: 0.8848\n",
      "Epoch 38/50, Loss: 0.0783843408824633, acc: 0.8868\n",
      "Epoch 39/50, Loss: 0.07536155516829561, acc: 0.8872\n",
      "Epoch 40/50, Loss: 0.07138698230809568, acc: 0.8942\n",
      "Epoch 41/50, Loss: 0.06506803584263643, acc: 0.8948\n",
      "Epoch 42/50, Loss: 0.06655888972075089, acc: 0.8902\n",
      "Epoch 43/50, Loss: 0.06692706501287649, acc: 0.8907\n",
      "Epoch 44/50, Loss: 0.06300862894519745, acc: 0.8961\n",
      "Epoch 45/50, Loss: 0.06591196397743414, acc: 0.8887\n",
      "Epoch 46/50, Loss: 0.05696566736854403, acc: 0.8885\n",
      "Epoch 47/50, Loss: 0.05302913849006224, acc: 0.8919\n",
      "Epoch 48/50, Loss: 0.053954730872684126, acc: 0.8964\n",
      "Epoch 49/50, Loss: 0.056237851863827015, acc: 0.8859\n",
      "Epoch 50/50, Loss: 0.04883448075969963, acc: 0.8888\n"
     ]
    }
   ],
   "source": [
    "def train_teacher(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        test_acc = test(model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "train_teacher(teacher_model, dataloader_train_global, 50, nn.CrossEntropyLoss(), torch.optim.Adam(teacher_model.parameters(), lr=0.001), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_distill(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs) \n",
    "            T=2.0\n",
    "            student_outputs = student_model(inputs)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion(student_outputs, teacher_outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "def linear_feature_logit(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_feature = teacher_model.forward_feature(inputs) \n",
    "                b, c, h, w=teacher_feature.shape\n",
    "                teacher_feature=teacher_feature.view(b, c, h*w).mean(-1)\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            T=2.0\n",
    "            student_feature = student_model.forward_feature(inputs)\n",
    "            b, c, h, w=student_feature.shape\n",
    "            student_feature=student_feature.view(b, c, h*w).mean(-1)\n",
    "            student_feature=student_model.linear(student_feature)\n",
    "            student_outputs=student_model(inputs)\n",
    "            teacher_feature=nn.functional.softmax(teacher_feature/T, dim=1)\n",
    "            student_feature=nn.functional.log_softmax(student_feature/T, dim=1)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion(student_outputs, teacher_outputs) + (T**2)*criterion(student_feature, teacher_feature)/2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "def orthogonal_feature_logit(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_feature = teacher_model.forward_feature(inputs) \n",
    "                b, c, h, w=teacher_feature.shape\n",
    "                teacher_feature=teacher_feature.view(b, c, h*w).mean(-1)\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            T=2.0\n",
    "            student_feature = student_model.forward_feature(inputs)\n",
    "            b, c, h, w=student_feature.shape\n",
    "            student_feature=student_feature.view(b, c, h*w).mean(-1)\n",
    "            student_feature=student_model.orthogonal(student_feature)\n",
    "            student_outputs=student_model(inputs)\n",
    "            teacher_feature=nn.functional.softmax(teacher_feature/T, dim=1)\n",
    "            student_feature=nn.functional.log_softmax(student_feature/T, dim=1)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion(student_outputs, teacher_outputs) + (T**2)*criterion(student_feature, teacher_feature)/2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.805564669868614, acc: 0.7637\n",
      "Epoch 2/50, Loss: 1.7319887687888327, acc: 0.7616\n",
      "Epoch 3/50, Loss: 1.6939975513687617, acc: 0.758\n",
      "Epoch 4/50, Loss: 1.6815914877607852, acc: 0.7617\n",
      "Epoch 5/50, Loss: 1.666155357904072, acc: 0.7641\n",
      "Epoch 6/50, Loss: 1.6313270258752606, acc: 0.7701\n",
      "Epoch 7/50, Loss: 1.5688068174485919, acc: 0.7685\n",
      "Epoch 8/50, Loss: 1.5256271558471872, acc: 0.7733\n",
      "Epoch 9/50, Loss: 1.5056571226708497, acc: 0.7805\n",
      "Epoch 10/50, Loss: 1.4792810444590412, acc: 0.7839\n",
      "Epoch 11/50, Loss: 1.4928666034454032, acc: 0.7734\n",
      "Epoch 12/50, Loss: 1.5759678496212899, acc: 0.787\n",
      "Epoch 13/50, Loss: 1.6601371327532997, acc: 0.7991\n",
      "Epoch 14/50, Loss: 1.642585719499407, acc: 0.7991\n",
      "Epoch 15/50, Loss: 1.601826298085949, acc: 0.8035\n",
      "Epoch 16/50, Loss: 1.5294582615547543, acc: 0.8153\n",
      "Epoch 17/50, Loss: 1.4637841989722433, acc: 0.7976\n",
      "Epoch 18/50, Loss: 1.4199930374758154, acc: 0.803\n",
      "Epoch 19/50, Loss: 1.352782128355171, acc: 0.8207\n",
      "Epoch 20/50, Loss: 1.2661211915031265, acc: 0.8247\n",
      "Epoch 21/50, Loss: 1.1824146446925174, acc: 0.8151\n",
      "Epoch 22/50, Loss: 1.1322929827095587, acc: 0.8146\n",
      "Epoch 23/50, Loss: 1.0901522825035868, acc: 0.8127\n",
      "Epoch 24/50, Loss: 1.0644051679704762, acc: 0.8134\n",
      "Epoch 25/50, Loss: 1.040126691702046, acc: 0.8071\n",
      "Epoch 26/50, Loss: 1.0234240001138253, acc: 0.8058\n",
      "Epoch 27/50, Loss: 1.0169605873053587, acc: 0.7986\n",
      "Epoch 28/50, Loss: 1.0275293105765233, acc: 0.8017\n",
      "Epoch 29/50, Loss: 1.0577184293843522, acc: 0.8039\n",
      "Epoch 30/50, Loss: 1.0778522465047957, acc: 0.7985\n",
      "Epoch 31/50, Loss: 1.1282307716864575, acc: 0.7905\n",
      "Epoch 32/50, Loss: 1.2815854953059667, acc: 0.7557\n",
      "Epoch 33/50, Loss: 1.3550289284579362, acc: 0.7268\n",
      "Epoch 34/50, Loss: 1.2913169049763982, acc: 0.7636\n",
      "Epoch 35/50, Loss: 1.2076242047397396, acc: 0.7565\n",
      "Epoch 36/50, Loss: 1.1706604425665699, acc: 0.773\n",
      "Epoch 37/50, Loss: 1.1693871907795532, acc: 0.7832\n",
      "Epoch 38/50, Loss: 1.1832760715786415, acc: 0.7954\n",
      "Epoch 39/50, Loss: 1.1432085074955904, acc: 0.8278\n",
      "Epoch 40/50, Loss: 1.0727064635934709, acc: 0.8304\n",
      "Epoch 41/50, Loss: 1.003722071647644, acc: 0.8349\n",
      "Epoch 42/50, Loss: 0.9410812473749812, acc: 0.825\n",
      "Epoch 43/50, Loss: 0.9078949942618986, acc: 0.8165\n",
      "Epoch 44/50, Loss: 0.8888105750083923, acc: 0.8068\n",
      "Epoch 45/50, Loss: 0.8877678664424752, acc: 0.8016\n",
      "Epoch 46/50, Loss: 0.8853238756898083, acc: 0.7783\n",
      "Epoch 47/50, Loss: 0.9103282810766485, acc: 0.7775\n",
      "Epoch 48/50, Loss: 0.9366439218762554, acc: 0.7864\n",
      "Epoch 49/50, Loss: 0.9751911438718627, acc: 0.7957\n",
      "Epoch 50/50, Loss: 1.0116897078254554, acc: 0.8002\n"
     ]
    }
   ],
   "source": [
    "#student_model=CNNCifar().to(device)\n",
    "logit_distill(teacher_model, student_model, dataloader_test_global, 50, nn.KLDivLoss(reduction='batchmean'), torch.optim.Adam(student_model.parameters(), lr=0.001), device, temperature=2.0, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6491282842581785, acc: 0.8586\n",
      "Epoch 2/20, Loss: 0.6230390667915344, acc: 0.8674\n",
      "Epoch 3/20, Loss: 0.6303821473936492, acc: 0.8564\n",
      "Epoch 4/20, Loss: 0.6190885980672474, acc: 0.8502\n",
      "Epoch 5/20, Loss: 0.5959386535083191, acc: 0.8494\n",
      "Epoch 6/20, Loss: 0.6252081926110424, acc: 0.8554\n",
      "Epoch 7/20, Loss: 0.638965550102765, acc: 0.8555\n",
      "Epoch 8/20, Loss: 0.6946149558960637, acc: 0.8477\n",
      "Epoch 9/20, Loss: 0.7039364338675632, acc: 0.8485\n",
      "Epoch 10/20, Loss: 0.6947954512095149, acc: 0.8615\n",
      "Epoch 11/20, Loss: 0.6857264796389809, acc: 0.8557\n",
      "Epoch 12/20, Loss: 0.6562785517565811, acc: 0.7911\n",
      "Epoch 13/20, Loss: 0.6995280151125751, acc: 0.8321\n",
      "Epoch 14/20, Loss: 0.7234813631335392, acc: 0.8109\n",
      "Epoch 15/20, Loss: 0.7122362146649179, acc: 0.7994\n",
      "Epoch 16/20, Loss: 0.6693531018269213, acc: 0.8006\n",
      "Epoch 17/20, Loss: 0.6834142453308347, acc: 0.8427\n",
      "Epoch 18/20, Loss: 0.6744329091868823, acc: 0.86\n",
      "Epoch 19/20, Loss: 0.6850139351585244, acc: 0.8511\n",
      "Epoch 20/20, Loss: 0.6588590205470218, acc: 0.8286\n"
     ]
    }
   ],
   "source": [
    "#student_model=CNNCifar().to(device)\n",
    "orthogonal_feature_logit(teacher_model, student_model, dataloader_test_global, 20, nn.KLDivLoss(reduction='batchmean'), torch.optim.Adam(student_model.parameters(), lr=0.001), device, temperature=2.0, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.6040977605536013, acc: 0.7746\n",
      "Epoch 2/50, Loss: 1.4876820956227146, acc: 0.781\n",
      "Epoch 3/50, Loss: 1.4266639442383489, acc: 0.7866\n",
      "Epoch 4/50, Loss: 1.3946480041817775, acc: 0.791\n",
      "Epoch 5/50, Loss: 1.3790809270701831, acc: 0.7876\n",
      "Epoch 6/50, Loss: 1.4334243166295788, acc: 0.7434\n",
      "Epoch 7/50, Loss: 1.5278609402572052, acc: 0.7387\n",
      "Epoch 8/50, Loss: 1.5920188702359985, acc: 0.7475\n",
      "Epoch 9/50, Loss: 1.5755640223056455, acc: 0.7292\n",
      "Epoch 10/50, Loss: 1.561286427552187, acc: 0.6944\n",
      "Epoch 11/50, Loss: 1.5968164363993873, acc: 0.747\n",
      "Epoch 12/50, Loss: 1.623670150962057, acc: 0.7781\n",
      "Epoch 13/50, Loss: 1.5176632019537915, acc: 0.7845\n",
      "Epoch 14/50, Loss: 1.4688349015350584, acc: 0.8032\n",
      "Epoch 15/50, Loss: 1.4361648619929446, acc: 0.8126\n",
      "Epoch 16/50, Loss: 1.3642199314847778, acc: 0.8145\n",
      "Epoch 17/50, Loss: 1.283259337838692, acc: 0.8125\n",
      "Epoch 18/50, Loss: 1.2238493595696702, acc: 0.8215\n",
      "Epoch 19/50, Loss: 1.1642638724061507, acc: 0.8254\n",
      "Epoch 20/50, Loss: 1.1402955296673352, acc: 0.8176\n",
      "Epoch 21/50, Loss: 1.1106617846820928, acc: 0.8199\n",
      "Epoch 22/50, Loss: 1.0916454863699177, acc: 0.8114\n",
      "Epoch 23/50, Loss: 1.0826638925679122, acc: 0.8066\n",
      "Epoch 24/50, Loss: 1.0961867435069024, acc: 0.7834\n",
      "Epoch 25/50, Loss: 1.0978780698172654, acc: 0.775\n",
      "Epoch 26/50, Loss: 1.0858688897724393, acc: 0.7894\n",
      "Epoch 27/50, Loss: 1.0918925159339663, acc: 0.8132\n",
      "Epoch 28/50, Loss: 1.080883057057103, acc: 0.7556\n",
      "Epoch 29/50, Loss: 1.1095638501493237, acc: 0.7365\n",
      "Epoch 30/50, Loss: 1.1028442790236654, acc: 0.7484\n",
      "Epoch 31/50, Loss: 1.0949434517305108, acc: 0.7742\n",
      "Epoch 32/50, Loss: 1.0550009461143348, acc: 0.7919\n",
      "Epoch 33/50, Loss: 1.0427191449871547, acc: 0.7969\n",
      "Epoch 34/50, Loss: 1.0373659216904942, acc: 0.8137\n",
      "Epoch 35/50, Loss: 1.0445775310449963, acc: 0.8155\n",
      "Epoch 36/50, Loss: 1.0351615978192679, acc: 0.8293\n",
      "Epoch 37/50, Loss: 1.0014933211893975, acc: 0.8386\n",
      "Epoch 38/50, Loss: 0.9460673362393922, acc: 0.8317\n",
      "Epoch 39/50, Loss: 0.9063119056481349, acc: 0.8253\n",
      "Epoch 40/50, Loss: 0.903172812884367, acc: 0.83\n",
      "Epoch 41/50, Loss: 0.923653794240348, acc: 0.8225\n",
      "Epoch 42/50, Loss: 0.9497974877116047, acc: 0.8195\n",
      "Epoch 43/50, Loss: 0.956189184249202, acc: 0.8188\n",
      "Epoch 44/50, Loss: 0.9650964299334756, acc: 0.8309\n",
      "Epoch 45/50, Loss: 0.9203561778309979, acc: 0.8223\n",
      "Epoch 46/50, Loss: 0.8844829080980036, acc: 0.8237\n",
      "Epoch 47/50, Loss: 0.8476230856738513, acc: 0.8162\n",
      "Epoch 48/50, Loss: 0.8512597159494327, acc: 0.8282\n",
      "Epoch 49/50, Loss: 0.8411743422097797, acc: 0.797\n",
      "Epoch 50/50, Loss: 0.8428529067130028, acc: 0.7986\n"
     ]
    }
   ],
   "source": [
    "#student_model=CNNCifar().to(device )\n",
    "linear_feature_logit(teacher_model, student_model, dataloader_test_global, 50, nn.KLDivLoss(reduction='batchmean'), torch.optim.Adam(student_model.parameters(), lr=0.001), device, temperature=2.0, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication Round 1/10\n",
      "Epoch 1/10, Loss: 1.2938785170350233, acc: 0.6189\n",
      "Epoch 2/10, Loss: 0.9349009609588271, acc: 0.6871\n",
      "Epoch 3/10, Loss: 0.8005198968950745, acc: 0.712\n",
      "Epoch 4/10, Loss: 0.7175334631024725, acc: 0.7507\n",
      "Epoch 5/10, Loss: 0.6621169444087827, acc: 0.7662\n",
      "Epoch 6/10, Loss: 0.6160329605459862, acc: 0.7723\n",
      "Epoch 7/10, Loss: 0.5832617718088048, acc: 0.7877\n",
      "Epoch 8/10, Loss: 0.5479911274617285, acc: 0.7965\n",
      "Epoch 9/10, Loss: 0.5205248168972142, acc: 0.7898\n",
      "Epoch 10/10, Loss: 0.49528206301772076, acc: 0.798\n",
      "Teacher training time: 127.73913908004761\n",
      "Epoch 1/10, Loss: 141.07748046102404, acc: 0.5244\n",
      "Epoch 2/10, Loss: 139.95803736433197, acc: 0.6249\n",
      "Epoch 3/10, Loss: 139.47818746446055, acc: 0.6865\n",
      "Epoch 4/10, Loss: 139.1932224321969, acc: 0.7205\n",
      "Epoch 5/10, Loss: 139.0360431912579, acc: 0.7367\n",
      "Epoch 6/10, Loss: 138.98013189774525, acc: 0.7385\n",
      "Epoch 7/10, Loss: 138.94964947277987, acc: 0.7414\n",
      "Epoch 8/10, Loss: 138.91169506990457, acc: 0.7489\n",
      "Epoch 9/10, Loss: 138.87577607360066, acc: 0.7638\n",
      "Epoch 10/10, Loss: 138.8386128099659, acc: 0.7626\n",
      "Distillation time: 197.3418323993683\n",
      "Communication Round 2/10\n",
      "Epoch 1/10, Loss: 0.4755638306555541, acc: 0.8104\n",
      "Epoch 2/10, Loss: 0.4576042096328248, acc: 0.8113\n",
      "Epoch 3/10, Loss: 0.43880968580922813, acc: 0.8149\n",
      "Epoch 4/10, Loss: 0.4229083687371915, acc: 0.826\n",
      "Epoch 5/10, Loss: 0.41006295287700567, acc: 0.813\n",
      "Epoch 6/10, Loss: 0.39007875151798854, acc: 0.8213\n",
      "Epoch 7/10, Loss: 0.3839923455129804, acc: 0.8214\n",
      "Epoch 8/10, Loss: 0.37007603605689904, acc: 0.8293\n",
      "Epoch 9/10, Loss: 0.35590101690853343, acc: 0.8284\n",
      "Epoch 10/10, Loss: 0.3413137425775723, acc: 0.8349\n",
      "Teacher training time: 128.84552145004272\n",
      "Epoch 1/10, Loss: 139.19510119474387, acc: 0.7731\n",
      "Epoch 2/10, Loss: 139.0649657430528, acc: 0.7809\n",
      "Epoch 3/10, Loss: 138.99609336370153, acc: 0.7959\n",
      "Epoch 4/10, Loss: 138.9545714945733, acc: 0.794\n",
      "Epoch 5/10, Loss: 138.92493255228936, acc: 0.7994\n",
      "Epoch 6/10, Loss: 138.89687936517257, acc: 0.797\n",
      "Epoch 7/10, Loss: 138.85570670984967, acc: 0.8018\n",
      "Epoch 8/10, Loss: 138.83613451221322, acc: 0.7991\n",
      "Epoch 9/10, Loss: 138.81557242477996, acc: 0.8041\n",
      "Epoch 10/10, Loss: 138.79678344726562, acc: 0.806\n",
      "Distillation time: 197.10632348060608\n",
      "Communication Round 3/10\n",
      "Epoch 1/10, Loss: 0.3336848758370675, acc: 0.8326\n",
      "Epoch 2/10, Loss: 0.32619846507411476, acc: 0.8339\n",
      "Epoch 3/10, Loss: 0.3140714396448696, acc: 0.8305\n",
      "Epoch 4/10, Loss: 0.30681408629240586, acc: 0.8331\n",
      "Epoch 5/10, Loss: 0.3011558744151269, acc: 0.8369\n",
      "Epoch 6/10, Loss: 0.2879939302992638, acc: 0.841\n",
      "Epoch 7/10, Loss: 0.28504745646968216, acc: 0.839\n",
      "Epoch 8/10, Loss: 0.2766826755707831, acc: 0.835\n",
      "Epoch 9/10, Loss: 0.26645528121143963, acc: 0.8307\n",
      "Epoch 10/10, Loss: 0.2576575005412712, acc: 0.8395\n",
      "Teacher training time: 129.48973321914673\n",
      "Epoch 1/10, Loss: 139.0981864446326, acc: 0.8174\n",
      "Epoch 2/10, Loss: 138.9532248581512, acc: 0.818\n",
      "Epoch 3/10, Loss: 138.8948333353936, acc: 0.8191\n",
      "Epoch 4/10, Loss: 138.88014955158476, acc: 0.8155\n",
      "Epoch 5/10, Loss: 138.86507831042326, acc: 0.8179\n",
      "Epoch 6/10, Loss: 138.85265050960493, acc: 0.8214\n",
      "Epoch 7/10, Loss: 138.83546911312055, acc: 0.8153\n",
      "Epoch 8/10, Loss: 138.83114005945905, acc: 0.8151\n",
      "Epoch 9/10, Loss: 138.82732091976118, acc: 0.8155\n",
      "Epoch 10/10, Loss: 138.82263144963903, acc: 0.8117\n",
      "Distillation time: 198.16468477249146\n",
      "Communication Round 4/10\n",
      "Epoch 1/10, Loss: 0.2564440875711953, acc: 0.8397\n",
      "Epoch 2/10, Loss: 0.24876915326203836, acc: 0.844\n",
      "Epoch 3/10, Loss: 0.24217216801993988, acc: 0.8395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Train teacher model\u001b[39;00m\n\u001b[0;32m     80\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 81\u001b[0m \u001b[43mtrain_teacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Distill knowledge to student model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain_teacher\u001b[1;34m(model, dataloader, epochs, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test(model, dataloader_test_global, device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training function for the teacher model\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def train_teacher(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        test_acc = test(model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Define the distillation function\n",
    "def distill(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    criterion1 = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_features = teacher_model.forward_feature(inputs)\n",
    "                teacher_outputs = teacher_model.forward_head(teacher_features) \n",
    "            T=2.0\n",
    "            student_features = student_model.forward_feature(inputs)\n",
    "            student_outputs = student_model.forward_head(student_features)\n",
    "            student_features = student_model.orthogonal_projector(student_features)\n",
    "            student_features=nn.functional.log_softmax(student_features/T, dim=1)\n",
    "            teacher_features=nn.functional.softmax(teacher_features/T, dim=1)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion1(student_features, teacher_features)+ (T**2)*criterion(student_outputs, teacher_outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# Define loss criterion and optimizers\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.KLDivLoss(reduction='batchmean')\n",
    "teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and distillation process\n",
    "communication_rounds = 10\n",
    "teacher_epochs = 10\n",
    "distill_epochs = 10\n",
    "\n",
    "for round in range(communication_rounds):\n",
    "    print(f\"Communication Round {round+1}/{communication_rounds}\")\n",
    "    # Train teacher model\n",
    "    start=time.time()\n",
    "    train_teacher(teacher_model, dataloader_train_global, teacher_epochs, criterion1, teacher_optimizer, device)\n",
    "    print(f\"Teacher training time: {time.time()-start}\")\n",
    "    # Distill knowledge to student model\n",
    "    distill(teacher_model, student_model, dataloader_test_global, distill_epochs, criterion2, student_optimizer, device)\n",
    "    print(f\"Distillation time: {time.time()-start}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
